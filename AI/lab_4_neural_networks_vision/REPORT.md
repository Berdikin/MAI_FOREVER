# Отчет по лабораторной работе 
## по курсу "Искусственый интеллект"

## Нейросетям для распознавания изображений


### Студенты: 

| ФИО       | Роль в проекте                     | Оценка       |
|-----------|------------------------------------|--------------|
| Бердикин Тимофей Алексеевич | Обучил полносвязную многослойную модель, написал отчёт |          |
| Лагода Дмитрий Артур Игоревич | Обработал датасет, обучил свёрточную модель|       |
| Драчёв Матвей Павлович| Обработал фотографии с символами, обучил полносвязную однослойную модель |      |

## Результат проверки

| Преподаватель     | Дата         |  Оценка       |
|-------------------|--------------|---------------|
| Сошников Д.В. |              |               |

> *Комментарии проверяющих (обратите внимание, что более подробные комментарии возможны непосредственно в репозитории по тексту программы)*

## Тема работы

Опишите тему работы, включая предназначенный для распознавания набор символов.

    Варианты рукописных символов (номер определяется следующим образом: ASCII-коды первых символов фамилий всех участников команды складываются и берется остаток от деления на 5 + 1)

Бердикин -> B -> 66

Драчёв -> D -> 68

Лагода -> L -> 76

(66 + 68 + 76) % 5 + 1 = 0 + 1 = 1

Следовательно, вариант 1: символы принадлежности множеству, пересечения множества, объединения множества и символ пустого множества.

Символ принадлежности множеству: ∈

Символ пересечения множества: ⋂

Символ объединения множества: ⋃

Символ пустого множества: ∅

## Распределение работы в команде

| ФИО       | Чем занимался                    |
|-----------|------------------------------------|
| Бердикин Тимофей Алексеевич | Обучил полносвязную многослойную модель, написал отчёт |
| Лагода Дмитрий Артур Игоревич | Вырезал изображения, обучил свёрточную модель|
| Драчёв Матвей Павлович| Обработал фотографии с символами, обучил полносвязную однослойную модель |

## Подготовка данных

[символы для обучающего набора:](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-berd-drach-and-lag/blob/master/train_1.jpg)

![текст](train_1.jpg)

[символы для обучающего набора:](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-berd-drach-and-lag/blob/master/train_2.jpg)

![текст](train_2.jpg)

[символы для тестировочного набора:]https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-berd-drach-and-lag/blob/master/test.jpg)
![текст](test.jpg)

Мы решили использовать шаблон листа в клеточку для того, чтобы фотографии символов были одинаковыми. Выписали по 400 раз каждый символ, итого получилось 1200 штук. Сфотографировали листы с символами и обработали с помощью онлайн-редактора на сайте photopea.ru (подкрутили ползунок яркости и резкости).

Чтобы было удобнее вырезать отдельные символы, мы обрезали отредактированные фотографии максимально близко к клеткам и сжали их, чтобы они были одного размера.
К сожалению, на момент выполнения лабораторной работы мы не догадались использовать одинаковые ручки для того, чтобы писать символы. Поэтому на фотографиях получились символы разной степени жирности. Это и помогло нам выбрать, какие фотографии для мы будем использовать для обучения, а какие - для теста. 

Но, конечно же, нейросеть, вырезавшая символы из середины фотографии, не могла работать безошибочно. Мы обнаружили, что она зачастую обращает больше внимания на клетки, чем на сами символы:
![](https://sun9-10.userapi.com/impg/HdveQA1gfin2IM6VxBjF8J9PA4FstyhmfeewIQ/-FLhzJS5MtY.jpg)
![](https://sun9-41.userapi.com/impg/tBE7S3YxYsHgszxgp9hwgKNS-D0PaePZZ5LCKA/vUWyjL2VAtM.jpg)

Пришлось убирать в Paint клетки, чтобы остались только символы.

Наши картинки - это квадраты с символами. Мы задаём для фотографии и вырезания картинок следующие параметры
dim - размерность фотографии, step = 32 - шаг при вырезе картинки, count = 400 - количество картинок (здесь же определяем количество классов - classes = 4)

```python
dim = (640, 640)
step = 32
count = 400
num_classes = 10
classes = 4

# 0 - принадлежность
# 1 - пересечение
# 2 - объединение
# 3 - пустое множество
```
И вот, используя OpenCV, мы сжимаем изображение по данным параметрам:

```python
Drachev_image = cv2.imread("test.jpg")
resized_Drachev_image = cv2.resize(Drachev_image, dim, interpolation = cv2.INTER_AREA)
```
Мы проходим фотографию по строкам и столбцам, вырезаем картинки 32х32 и записываем (в данном примере) в созданный список:

```python
list_Drachev_im = []

for i in range(0, resized_Drachev_image.shape[0], step):
    for j in range(0, resized_Drachev_image.shape[0], step):
        list_Drachev_im.append(resized_Drachev_image[i:i + step, j:j + step])
```

[Ссылка на получившийся ноутбук](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-berd-drach-and-lag/blob/master/lab2AI.ipynb)
## Загрузка данных

Создаем два массива (из тренировочной выборки и тестовочной выборки) numpy:

```python
x_train = np.array(list_Lagoda_im + list_Berdikin_im)
x_test = np.array(list_Drachev_im)
```

Готовим датасеты: картинки - это многомерные массивы, где каждый пиксель - входной признак. Нормируем, приводя массивы к типу float32 и деля на 255, чтобы каждое значение было в интервале от 0 до 1. И приводим обучающие данные к категориальным признакам (преобразуем в прямое кодирование (one-hot encoding) с помощью встроенной функции).

```python
x_train_norm = x_train.astype('float32')
x_test_norm = x_test.astype('float32')
x_train_norm /= 255
x_test_norm /= 255
print('x_train shape:', x_train_norm.shape)
print(x_train_norm.shape[0], 'train samples')
print(x_test_norm.shape[0], 'test samples')

y_train_ = keras.utils.to_categorical(y_train, classes)
y_test_ = keras.utils.to_categorical(y_test, classes)
```

## Обучение нейросети

### Полносвязная однослойная сеть
**Архитектура**
На вход подаются картинки - это самый верхний слой, inp. Так как зто однослойная сеть, то скрытых слоев нет, и поэтому далее идет выходной слой, состоящий из 4 нейронов и активирующийся softmax'сом.
Затем определяем модель от входных и выходных данных, компилируем, используя категориальную кросс-энтропию в качестве функции потерь, оптимайзер Adam и метрику 
accuracy.

```python
hidden_size = 128

inp = Input(shape = (32,32,3))
flat = Flatten()(inp)
out = Dense(classes, activation='softmax')(flat)

model_ogurec = Model(inputs = inp, outputs = out) #определяем модель
model_ogurec.compile(loss = keras.losses.categorical_crossentropy,#компилируем
              optimizer = keras.optimizers.Adam(),
              metrics = ['accuracy'])

model_ogurec.summary()
hist__ = model_ogurec.fit(x_train_norm, y_train_, #трениров очка
          batch_size = batch_size,
          epochs = 25, 
          verbose = 1, validation_data = (x_test_norm, y_test_)) #показали, где у нас тестовые наборы validation_data = (x_test_norm, y_test_)
```

**Результаты**
```
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 32, 32, 3)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 3072)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 4)                 12292     
=================================================================
Total params: 12,292
Trainable params: 12,292
Non-trainable params: 0
_________________________________________________________________
Train on 800 samples, validate on 400 samples
Epoch 1/25
800/800 [==============================] - 0s 199us/step - loss: 2.0250 - accuracy: 0.2925 - val_loss: 1.4928 - val_accuracy: 0.2275
Epoch 2/25
800/800 [==============================] - 0s 56us/step - loss: 1.5042 - accuracy: 0.3088 - val_loss: 1.5456 - val_accuracy: 0.2525
Epoch 3/25
800/800 [==============================] - 0s 44us/step - loss: 1.3555 - accuracy: 0.3963 - val_loss: 1.3174 - val_accuracy: 0.3000
Epoch 4/25
800/800 [==============================] - 0s 51us/step - loss: 1.1873 - accuracy: 0.4725 - val_loss: 1.2709 - val_accuracy: 0.3450
Epoch 5/25
800/800 [==============================] - 0s 43us/step - loss: 1.0839 - accuracy: 0.5788 - val_loss: 1.2736 - val_accuracy: 0.3425
Epoch 6/25
800/800 [==============================] - 0s 57us/step - loss: 1.0256 - accuracy: 0.6025 - val_loss: 1.0301 - val_accuracy: 0.6925
Epoch 7/25
800/800 [==============================] - 0s 54us/step - loss: 0.9020 - accuracy: 0.7237 - val_loss: 0.9646 - val_accuracy: 0.7175
Epoch 8/25
800/800 [==============================] - 0s 52us/step - loss: 0.8416 - accuracy: 0.7500 - val_loss: 0.9533 - val_accuracy: 0.6150
Epoch 9/25
800/800 [==============================] - 0s 55us/step - loss: 0.7847 - accuracy: 0.7887 - val_loss: 0.8960 - val_accuracy: 0.7075
Epoch 10/25
800/800 [==============================] - 0s 52us/step - loss: 0.7469 - accuracy: 0.7725 - val_loss: 0.9414 - val_accuracy: 0.5775
Epoch 11/25
800/800 [==============================] - 0s 46us/step - loss: 0.7458 - accuracy: 0.7462 - val_loss: 0.9045 - val_accuracy: 0.6500
Epoch 12/25
800/800 [==============================] - 0s 48us/step - loss: 0.7106 - accuracy: 0.7675 - val_loss: 0.8403 - val_accuracy: 0.6675
Epoch 13/25
800/800 [==============================] - 0s 62us/step - loss: 0.6657 - accuracy: 0.7987 - val_loss: 0.7260 - val_accuracy: 0.7950
Epoch 14/25
800/800 [==============================] - 0s 52us/step - loss: 0.6260 - accuracy: 0.8300 - val_loss: 0.7694 - val_accuracy: 0.6875
Epoch 15/25
800/800 [==============================] - 0s 59us/step - loss: 0.6100 - accuracy: 0.8075 - val_loss: 0.6945 - val_accuracy: 0.7950
Epoch 16/25
800/800 [==============================] - 0s 47us/step - loss: 0.5877 - accuracy: 0.8313 - val_loss: 0.6684 - val_accuracy: 0.8075
Epoch 17/25
800/800 [==============================] - 0s 35us/step - loss: 0.5675 - accuracy: 0.8363 - val_loss: 0.6833 - val_accuracy: 0.7900
Epoch 18/25
800/800 [==============================] - 0s 53us/step - loss: 0.5634 - accuracy: 0.8363 - val_loss: 0.6496 - val_accuracy: 0.8200
Epoch 19/25
800/800 [==============================] - 0s 44us/step - loss: 0.5601 - accuracy: 0.8188 - val_loss: 0.6904 - val_accuracy: 0.7400
Epoch 20/25
800/800 [==============================] - 0s 57us/step - loss: 0.5608 - accuracy: 0.8363 - val_loss: 0.6511 - val_accuracy: 0.7775
Epoch 21/25
800/800 [==============================] - 0s 48us/step - loss: 0.5413 - accuracy: 0.8250 - val_loss: 0.5605 - val_accuracy: 0.8625
Epoch 22/25
800/800 [==============================] - 0s 45us/step - loss: 0.4908 - accuracy: 0.8750 - val_loss: 0.5674 - val_accuracy: 0.8400
Epoch 23/25
800/800 [==============================] - 0s 54us/step - loss: 0.4936 - accuracy: 0.8637 - val_loss: 0.5317 - val_accuracy: 0.8575
Epoch 24/25
800/800 [==============================] - 0s 47us/step - loss: 0.4790 - accuracy: 0.8700 - val_loss: 0.5664 - val_accuracy: 0.8300
Epoch 25/25
800/800 [==============================] - 0s 45us/step - loss: 0.4781 - accuracy: 0.8662 - val_loss: 0.5234 - val_accuracy: 0.8550
```

### Полносвязная многослойная сеть
**Архитектура**

Задаем входные и выходные данные: входной слой, два скрытых слоя (по 32 нейрона каждый) и выходной слой.
На всех слоях, кроме входного полносвязного, используется функция активации ReLU, на выходном слое - softmax.

Определяем модель от входных и выходных данных, компилируем, используя категориальную кросс-энтропию в качестве функции потерь, оптимайзер Adam и метрику 
accuracy.

Плюсы и минусы функции ReLU:

   Плюсы:

  + по сравнению с вычислением сигмоида и гиперболического тангенса, требующих выполнения ресурсоемких операций (например, возведение в степень), ReLU может быть реализован с помощью простого порогового преобразования матрицы активаций в нуле
  
  + не подвержен насыщению
  
  + Применение ReLU существенно повышает скорость сходимости стохастического градиентного спуска (в некоторых случаях до 6 раз) по сравнению с сигмоидой и гиперболическим тангенсом

    Минусы:
  
  - ReLU не всегда достаточно надежна и в процессе обучения может выходить из строя (например, большой градиент, проходящий через ReLU, может привести к такому обновлению весов, что данный нейрон никогда больше не активируется, а следовательно будет необратимо выведен из строя)

Функция активации Softmax специально предназначена для мультиклассовой классификации и обеспечивает, чтобы сумма выходных значений всех нейронов слоя равна единице.

```python
hidden_size = 128

inp = Input(shape = (32,32,3))
hidden_1 = Dense(32, activation='relu')(inp)
hidden_2 = Dense(32, activation='relu')(hidden_1)
flat = Flatten()(hidden_2)
out = Dense(classes, activation='softmax')(flat)

model_ogurec = Model(inputs = inp, outputs = out) #определяем модель
model_ogurec.compile(loss = keras.losses.categorical_crossentropy,#компилируем
              optimizer = keras.optimizers.Adam(),
              metrics = ['accuracy'])

model_ogurec.summary()
hist_ = model_ogurec.fit(x_train_norm, y_train_, #трениров очка
          batch_size = batch_size,
          epochs = epochs, 
          verbose = 1, validation_data = (x_test_norm, y_test_)) #показали где у нас тестовые наборы validation_data = (x_test_norm, y_test_)
```

**Результаты**

```
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 32, 32, 3)         0         
_________________________________________________________________
dense_2 (Dense)              (None, 32, 32, 32)        128       
_________________________________________________________________
dense_3 (Dense)              (None, 32, 32, 32)        1056      
_________________________________________________________________
flatten_2 (Flatten)          (None, 32768)             0         
_________________________________________________________________
dense_4 (Dense)              (None, 4)                 131076    
=================================================================
Total params: 132,260
Trainable params: 132,260
Non-trainable params: 0
_________________________________________________________________
Train on 800 samples, validate on 400 samples
Epoch 1/18
800/800 [==============================] - 1s 768us/step - loss: 2.0084 - accuracy: 0.2988 - val_loss: 1.8680 - val_accuracy: 0.3875
Epoch 2/18
800/800 [==============================] - 0s 584us/step - loss: 1.7369 - accuracy: 0.3550 - val_loss: 1.4092 - val_accuracy: 0.2500
Epoch 3/18
800/800 [==============================] - 1s 639us/step - loss: 1.3192 - accuracy: 0.3750 - val_loss: 1.4099 - val_accuracy: 0.4500
Epoch 4/18
800/800 [==============================] - 1s 678us/step - loss: 1.1602 - accuracy: 0.5188 - val_loss: 1.1022 - val_accuracy: 0.7225
Epoch 5/18
800/800 [==============================] - 0s 541us/step - loss: 1.0022 - accuracy: 0.6825 - val_loss: 1.0663 - val_accuracy: 0.6400
Epoch 6/18
800/800 [==============================] - 0s 554us/step - loss: 0.8851 - accuracy: 0.7950 - val_loss: 0.9102 - val_accuracy: 0.7450
Epoch 7/18
800/800 [==============================] - 0s 578us/step - loss: 0.7711 - accuracy: 0.7837 - val_loss: 0.9192 - val_accuracy: 0.6025
Epoch 8/18
800/800 [==============================] - 1s 633us/step - loss: 0.6802 - accuracy: 0.8000 - val_loss: 0.7291 - val_accuracy: 0.7525
Epoch 9/18
800/800 [==============================] - 0s 580us/step - loss: 0.5845 - accuracy: 0.8250 - val_loss: 0.5888 - val_accuracy: 0.8425
Epoch 10/18
800/800 [==============================] - 0s 541us/step - loss: 0.5178 - accuracy: 0.8487 - val_loss: 0.5405 - val_accuracy: 0.8300
Epoch 11/18
800/800 [==============================] - 0s 561us/step - loss: 0.4534 - accuracy: 0.8525 - val_loss: 0.4996 - val_accuracy: 0.8350
Epoch 12/18
800/800 [==============================] - 0s 579us/step - loss: 0.4112 - accuracy: 0.8712 - val_loss: 0.4348 - val_accuracy: 0.8575
Epoch 13/18
800/800 [==============================] - 0s 537us/step - loss: 0.3785 - accuracy: 0.8800 - val_loss: 0.3698 - val_accuracy: 0.9000
Epoch 14/18
800/800 [==============================] - 0s 588us/step - loss: 0.3479 - accuracy: 0.8938 - val_loss: 0.3414 - val_accuracy: 0.8875
Epoch 15/18
800/800 [==============================] - 1s 776us/step - loss: 0.3252 - accuracy: 0.9000 - val_loss: 0.2887 - val_accuracy: 0.9200
Epoch 16/18
800/800 [==============================] - 1s 765us/step - loss: 0.3046 - accuracy: 0.9100 - val_loss: 0.2978 - val_accuracy: 0.9200
Epoch 17/18
800/800 [==============================] - 1s 663us/step - loss: 0.2846 - accuracy: 0.9250 - val_loss: 0.3608 - val_accuracy: 0.8825
Epoch 18/18
800/800 [==============================] - 0s 555us/step - loss: 0.2530 - accuracy: 0.9237 - val_loss: 0.2585 - val_accuracy: 0.9350
```

### Свёрточная сеть
**Архитектура** 

Используется модель последователього типа:

- используются сверточные фильтры 3х3;

- создаётся сверточный слой на 8 нейронов с размером ядра 3, шагом 1, функцией активации ReLU и нужным размером input_shape;

- слой MaxPooling2D с размером ядра 3 и шагом 2;

- сверточный слой на 16 нейронов с размером ядра 3, шагом 1, функцией активации ReLU и нужным размером input_shape;

- слой MaxPooling2D с размером ядра 3 и шагом 2;

- между слоем MaxPooling2D и Dense (слой из 4 нейронов с функцией активации softmax) находится слой выравнивания (Flatten). Он служит соединительным узлом между слоями;

- слой из 4 нейронов с функцией активации softmax.

А далее компилируем, используя категориальную кросс-энтропию в качестве функции потерь, оптимайзер Adam и метрику accuracy.

```python
model_conv_2 = Sequential()
model_conv_2.add(Conv2D(8, kernel_size=(3, 3), 
                strides=(1,1),
                activation='relu',
                input_shape=input_shape))
model_conv_2.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))
model_conv_2.add(Conv2D(16, kernel_size=(3, 3), 
                strides=(1,1),
                activation='relu',
                input_shape=input_shape))
model_conv_2.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))
model_conv_2.add(Flatten())
model_conv_2.add(Dense(classes, activation='softmax'))

model_conv_2.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adam(),
              metrics=['accuracy'])
model_conv_2.summary()
hist = model_conv_2.fit(x_train_norm, y_train_,
          batch_size=batch_size,
          epochs=25,
          verbose=1,
          validation_data=(x_test_norm, y_test_))
```

**Результаты**

```
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 30, 30, 8)         224       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 14, 14, 8)         0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 12, 12, 16)        1168      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 5, 16)          0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 400)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 4)                 1604      
=================================================================
Total params: 2,996
Trainable params: 2,996
Non-trainable params: 0
_________________________________________________________________
Train on 800 samples, validate on 400 samples
Epoch 1/25
800/800 [==============================] - 0s 446us/step - loss: 1.3987 - accuracy: 0.2912 - val_loss: 1.3727 - val_accuracy: 0.3675
Epoch 2/25
800/800 [==============================] - 0s 340us/step - loss: 1.3480 - accuracy: 0.5350 - val_loss: 1.3339 - val_accuracy: 0.6425
Epoch 3/25
800/800 [==============================] - 0s 279us/step - loss: 1.3075 - accuracy: 0.6762 - val_loss: 1.2881 - val_accuracy: 0.6075
Epoch 4/25
00/800 [==============================] - 0s 291us/step - loss: 1.2486 - accuracy: 0.7688 - val_loss: 1.2193 - val_accuracy: 0.7325
Epoch 5/25
800/800 [==============================] - 0s 289us/step - loss: 1.1715 - accuracy: 0.7375 - val_loss: 1.1285 - val_accuracy: 0.7425
Epoch 6/25
800/800 [==============================] - 0s 281us/step - loss: 1.0588 - accuracy: 0.7700 - val_loss: 1.0002 - val_accuracy: 0.7450
Epoch 7/25
800/800 [==============================] - 0s 273us/step - loss: 0.9151 - accuracy: 0.8288 - val_loss: 0.8503 - val_accuracy: 0.7775
Epoch 8/25
800/800 [==============================] - 0s 281us/step - loss: 0.7402 - accuracy: 0.8963 - val_loss: 0.6804 - val_accuracy: 0.8250
Epoch 9/25
800/800 [==============================] - 0s 355us/step - loss: 0.5601 - accuracy: 0.9112 - val_loss: 0.5176 - val_accuracy: 0.8700
Epoch 10/25
800/800 [==============================] - 0s 364us/step - loss: 0.4058 - accuracy: 0.9275 - val_loss: 0.3879 - val_accuracy: 0.9050
Epoch 11/25
800/800 [==============================] - 0s 358us/step - loss: 0.2942 - accuracy: 0.9500 - val_loss: 0.2826 - val_accuracy: 0.9450
Epoch 12/25
800/800 [==============================] - 0s 370us/step - loss: 0.2231 - accuracy: 0.9538 - val_loss: 0.2282 - val_accuracy: 0.9575
Epoch 13/25
800/800 [==============================] - 0s 372us/step - loss: 0.1751 - accuracy: 0.9600 - val_loss: 0.1526 - val_accuracy: 0.9675
Epoch 14/25
800/800 [==============================] - 0s 334us/step - loss: 0.1449 - accuracy: 0.9638 - val_loss: 0.1298 - val_accuracy: 0.9800
Epoch 15/25
800/800 [==============================] - 0s 279us/step - loss: 0.1176 - accuracy: 0.9675 - val_loss: 0.1475 - val_accuracy: 0.9675
Epoch 16/25
800/800 [==============================] - 0s 301us/step - loss: 0.1012 - accuracy: 0.9712 - val_loss: 0.1105 - val_accuracy: 0.9800
Epoch 17/25
800/800 [==============================] - 0s 328us/step - loss: 0.0854 - accuracy: 0.9762 - val_loss: 0.0884 - val_accuracy: 0.9900
Epoch 18/25
800/800 [==============================] - 0s 354us/step - loss: 0.0747 - accuracy: 0.9787 - val_loss: 0.1366 - val_accuracy: 0.9675
Epoch 19/25
800/800 [==============================] - 0s 355us/step - loss: 0.0656 - accuracy: 0.9837 - val_loss: 0.1109 - val_accuracy: 0.9775
Epoch 20/25
800/800 [==============================] - 0s 357us/step - loss: 0.0564 - accuracy: 0.9887 - val_loss: 0.1001 - val_accuracy: 0.9800
Epoch 21/25
800/800 [==============================] - 0s 399us/step - loss: 0.0503 - accuracy: 0.9900 - val_loss: 0.1258 - val_accuracy: 0.9650
Epoch 22/25
800/800 [==============================] - 0s 378us/step - loss: 0.0444 - accuracy: 0.9912 - val_loss: 0.1040 - val_accuracy: 0.9750
Epoch 23/25
800/800 [==============================] - 0s 321us/step - loss: 0.0399 - accuracy: 0.9937 - val_loss: 0.1042 - val_accuracy: 0.9675
Epoch 24/25
800/800 [==============================] - 0s 412us/step - loss: 0.0361 - accuracy: 0.9937 - val_loss: 0.0957 - val_accuracy: 0.9775
Epoch 25/25
800/800 [==============================] - 0s 311us/step - loss: 0.0330 - accuracy: 0.9937 - val_loss: 0.0904 - val_accuracy: 0.9800
```

## Выводы

Выполняя эту работу, мы познакомились с основами работы в TensorFlow и Keras при проектировании свёрточной, однослойной и полносвязной нейросетей. Мы получили опыт в подготовке данных и оценке результатов моделей, проводя коррекцию архитектуры сети. Данная работа дала нам начальное понимание понятия "нейронная сеть".

Основной сложностью при выполнении работы оказалась подготовка данных, так как от точности и информативности данных всегда зависит результат обучения моделей.

Командную работу мы осуществляли с помощью общей "беседы" в социальной сети ВКонтакте, а также через Discord. Нам было очень удобно таким образом осуществлять мгновенную коммуникацию друг с другом. Правда, возникли некоторые трудности из-за разных ручек, которыми мы писали текст, это описано в лабораторной.

Таким образом, данная работа дала не только теоретические и практические знания по предмету, но и опыт в разработке в команде, причём дистанционно.
