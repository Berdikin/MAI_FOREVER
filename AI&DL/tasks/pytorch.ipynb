{"cells":[{"cell_type":"markdown","metadata":{"id":"v5U0OQo1ZJAC"},"source":["PyTorch practice "]},{"cell_type":"markdown","metadata":{"id":"RmNTh6Z1ZStq"},"source":["PyTorch 101 basic knowledges "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTbVELEzZFlm"},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","print(f'Current PyTorch version: {torch.__version__}')\n","\n","data = [[1, 2, 3],[4, 5, 6],[7, 8, 9]]\n","x_data = torch.tensor(data)\n","\n","np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)\n","\n","x_ones = torch.ones_like(x_data)\n","x_rand = torch.rand_like(x_data, dtype=torch.float)\n","\n","shape = (2,3,)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n","\n","tensor = torch.rand(3,4)\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")\n"]},{"cell_type":"markdown","metadata":{"id":"R_VaEoKHxLYa"},"source":["CUDA Tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydPvGtSrZIBL"},"outputs":[],"source":["import torch\n","print(f'GPU is available: {torch.cuda.is_available()}')\n","print(f'count GPU devices: {torch.cuda.device_count()}')\n","\n","tensor = torch.randn((3,3))\n","print(tensor)\n","tensor = tensor.to(torch.int32)\n","print(tensor)\n","\n","tensor = torch.rand((4, 3), dtype=torch.double)\n","print(tensor)\n","ten_2 = torch.ones((2,2)).to(tensor)\n","print(ten_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwEVuvhbHH8V"},"outputs":[],"source":["tensor = torch.rand((4,3,7,7))\n","print(f'Tensor is on {tensor.get_device()} device')"]},{"cell_type":"code","execution_count":128,"metadata":{"id":"Y-dpVMTAHX9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor is on 0 device\n","Tensor is on 0 device\n","Tensor is on 0 device\n","Tensor is on 0 device\n"]}],"source":["tensor = torch.rand((4,3,7,7))\n","tensor = tensor.to('cuda:0')\n","print(f'Tensor is on {tensor.get_device()} device')\n","\n","tensor = torch.rand((4,3,7,7))\n","device = torch.device('cuda')\n","tensor = tensor.to(device)\n","print(f'Tensor is on {tensor.get_device()} device')\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","tensor = torch.rand((4,3,7,7))\n","tensor = tensor.to(device)\n","print(f'Tensor is on {tensor.get_device()} device')\n","\n","ten_2 = torch.rand((4,3,7,7)).to(tensor.device)\n","print(f'Tensor is on {tensor.get_device()} device')"]},{"cell_type":"code","execution_count":129,"metadata":{"id":"IoU4iBXkHgWr"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor is on 0 device, data type: torch.float64, grad: False\n"]}],"source":["tensor = torch.rand((3,2), dtype=torch.double, device=device, requires_grad=True)\n","ten_2 = torch.zeros(2,4).to(tensor)\n","print(f'Tensor is on {ten_2.get_device()} device, data type: {ten_2.dtype}, grad: {ten_2.requires_grad}')"]},{"cell_type":"code","execution_count":132,"metadata":{"id":"uPAdY8qbHPvL"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU device: <torch.cuda.device_of object at 0x7fd9a17a1080>\n","GPU capability: (7, 5)\n","GPU properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=11019MB, multi_processor_count=68)\n"]}],"source":["print(f'GPU device: {torch.cuda.device_of(tensor)}')\n","#print(f'GPU SM_ARCH: {torch.cuda.get_arch_list()}')\n","print(f'GPU capability: {torch.cuda.get_device_capability()}')\n","print(f'GPU properties: {torch.cuda.get_device_properties(0)}')\n","#print(f'GPU is initialized: {torch.cuda.is_initialized()}')"]},{"cell_type":"markdown","metadata":{},"source":["CUDA Module"]},{"cell_type":"code","execution_count":142,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":6493,"status":"error","timestamp":1638958768366,"user":{"displayName":"Andriy Sorokin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08930791557530031808"},"user_tz":-180},"id":"RgLkxEqnHR6E","outputId":"930af098-9f78-454b-a26c-f971a6588636"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'state': {140572672483048: {'step': 0, 'sum': tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]], device='cuda:0')}, 140572672483120: {'step': 0, 'sum': tensor([0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 140572672483192: {'step': 0, 'sum': tensor([[0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')}, 140572672483264: {'step': 0, 'sum': tensor([0.], device='cuda:0')}}, 'param_groups': [{'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0, 'initial_accumulator_value': 0, 'params': [140572672483048, 140572672483120, 140572672483192, 140572672483264]}]}\n","Model cuda:3\n","layer1.weight cuda:3\n","layer1.bias cuda:3\n","layer2.weight cuda:3\n","layer2.bias cuda:3\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","class Arch(torch.nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.layer1 = torch.nn.Linear(3, 7)\n","    self.layer2 = torch.nn.Linear(7, 1)\n","    \n","  def forward(self, x):\n","    x = F.tanh(layer1)\n","    x = F.tanh(layer2)\n","    return x\n","\n","\n","model = Arch()\n","model.to('cuda')\n","optim = torch.optim.Adagrad(model.parameters())\n","#print(optim.state_dict())\n","\n","'''\n","That’s not possible. Modules can hold parameters of different types on different \n","devices, and so it’s not always possible to unambiguously determine the device.\n","'''\n","print(f'Model {next(model.parameters()).device}')  \n","for n, p in model.named_parameters():\n","  print(n, p.device) "]},{"cell_type":"code","execution_count":143,"metadata":{"id":"zHtqV_GAR0Zv"},"outputs":[],"source":["class LayerModule_1(torch.nn.Module):\r\n","  def __init__(self):\r\n","    super().__init__()\r\n","    self.layer1 = torch.nn.Linear(4, 8)\r\n","    self.layer2 = torch.nn.Linear(8, 2)\r\n","    \r\n","  def forward(self, x):\r\n","    x = F.tanh(layer1)\r\n","    x = F.tanh(layer2)\r\n","    return x\r\n","\r\n","class LayerModule_2(torch.nn.Module):\r\n","  def __init__(self):\r\n","    super().__init__()\r\n","    self.op = torch.nn.Transformer()\r\n","    \r\n","  def forward(self, x):\r\n","    x = self.op(x)\r\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class NewArch(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.features = LayerModule_1()\n","        self.data = LayerModule_2()\n","\n","    def forward(self, x):\n","      x = self.features(x)\n","      x = self.data(x)\n","      return x\n","        \n","model = NewArch()        \n","# for n, p in model.named_parameters():\n","#   print(f'Layers names: {n}')\n","# print(f'modules: {model.modules}')\n","# for idx, m in enumerate(model.named_modules()):\n","#         print(idx, '->', m) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.add_module('Module_LSTM', torch.nn.LSTM(10, 2))  \n","for n, p in model.named_parameters():\n","   print(f'Layers names: {n}')\n","print(f'Model state_dict {model.state_dict()}')"]},{"cell_type":"markdown","metadata":{"id":"UQg7n4DmpQ7w"},"source":["Autograd"]},{"cell_type":"code","execution_count":151,"metadata":{"id":"Fj8rzSMw1X70"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requires grad: False, False\n","Tensor contents: tensor([[0.9908, 0.1574, 0.4543],\n","        [0.1250, 0.8412, 0.0419]]), tensor([[0.3362, 0.0197, 0.5008, 0.5163],\n","        [0.1634, 0.7573, 0.7735, 0.7368],\n","        [0.0629, 0.7425, 0.5592, 0.2885]])\n","Leaf tensor: True, True\n","Gradients: None, None\n","Grad function: None, None\n"]}],"source":["t1 = torch.rand((2,3))\n","t2 = torch.rand((3,4))\n","print(f'Requires grad: {t1.requires_grad}, {t2.requires_grad}')\n","print(f'Tensor contents: {t1}, {t2}')\n","print(f'Leaf tensor: {t1.is_leaf}, {t2.is_leaf}')\n","print(f'Gradients: {t1.grad}, {t2.grad}')\n","print(f'Grad function: {t1.grad_fn}, {t2.grad_fn}')"]},{"cell_type":"code","execution_count":152,"metadata":{"id":"zrtLs1VQ1YEy"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requires grad: False\n","Tensor contents: tensor([[0.3874, 0.4759, 0.8719, 0.7586],\n","        [0.1821, 0.6706, 0.7366, 0.6964]])\n","Leaf tensor: True\n","Gradients: None\n","Grad function: None\n"]}],"source":["t3 = torch.mm(t1,t2)\n","print(f'Requires grad: {t3.requires_grad}')\n","print(f'Tensor contents: {t3}')\n","print(f'Leaf tensor: {t3.is_leaf}')\n","print(f'Gradients: {t3.grad}')\n","print(f'Grad function: {t3.grad_fn}')"]},{"cell_type":"code","execution_count":155,"metadata":{"id":"TH46W0Fg1YLc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requires grad: True, True, True\n","Tensor contents: tensor([[0.8513, 0.9243, 0.8080],\n","        [0.2901, 0.6435, 0.0570],\n","        [0.8803, 0.2969, 0.8275],\n","        [0.4659, 0.3326, 0.8795]], requires_grad=True), tensor([[1.5888, 1.1755, 1.7289],\n","        [1.3225, 1.0502, 1.4075]], grad_fn=<MmBackward>), tensor([[0.8350, 2.2051, 2.6716, 2.1852],\n","        [0.7048, 1.8663, 2.2616, 1.8626]], grad_fn=<MmBackward>)\n","Leaf tensor: True, False, False\n","Grad function: None, <MmBackward object at 0x7fd9a17d3278>, <MmBackward object at 0x7fd9a17d3278>\n"]}],"source":["t4 = torch.rand((4,3), requires_grad=True)\n","t5 = torch.mm(t3,t4)\n","t6 = torch.mm(t5,t2)\n","print(f'Requires grad: {t4.requires_grad}, {t5.requires_grad}, {t6.requires_grad}')\n","print(f'Tensor contents: {t4}, {t5}, {t6}')\n","print(f'Leaf tensor: {t4.requires_grad}, {t5.is_leaf}, {t6.is_leaf}')\n","#print(f'Gradients: {t4.grad}, {t5.grad}, {t6.grad}')\n","print(f'Grad function: {t4.grad_fn}, {t5.grad_fn}, {t6.grad_fn}')"]},{"cell_type":"code","execution_count":158,"metadata":{"id":"NL38VYhcpTTm"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gradient function for output = <AddBackward0 object at 0x7fd9a062a908>\n","Gradient function for loss = <L1LossBackward object at 0x7fd9a177e198>\n","Wghts gradients: tensor([[0.1747, 0.1747],\n","        [0.0310, 0.0310],\n","        [0.2693, 0.2693],\n","        [0.3915, 0.3915],\n","        [0.0824, 0.0824],\n","        [0.3638, 0.3638],\n","        [0.2762, 0.2762]])\n","Bias gradients: tensor([0.5000, 0.5000])\n","[input, label, wght, bias, output]\n","[False, False, True, True, False]\n"]}],"source":["'''\n","To optimize weights of parameters in the neural network, we need to compute the \n","derivatives of our loss function with respect to parameters. To compute those \n","derivatives, we call loss.backward(), and then retrieve the values from \n","appropriate parameters by .grad attribute:\n","'''\n","input = torch.rand(7)\n","label = torch.rand(2)\n","weights = torch.rand((7,2), requires_grad=True)\n","biases = torch.rand(2, requires_grad=True)\n","output = torch.matmul(input, weights) + biases\n","loss = torch.nn.functional.l1_loss(output, label)\n","print(f'Gradient function for output = {output.grad_fn}')\n","print(f'Gradient function for loss = {loss.grad_fn}')\n","#loss.backward(retain_graph=True)\n","loss.backward()\n","print(f'Wghts gradients: {weights.grad}')\n","print(f'Bias gradients: {biases.grad}')\n","'''We can only obtain the grad properties for the leaf nodes of the computational graph, \n","which have requires_grad property set to True\n","'''\n","print(f'[input, label, wght, bias, output]')\n","print(list(map(lambda x: x.is_leaf and x.requires_grad, [input, label, weights, biases, output])))"]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1638895186379,"user":{"displayName":"Andriy Sorokin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08930791557530031808"},"user_tz":-180},"id":"1Yrki3sCw5wM","outputId":"7c1430f8-9c34-4972-eadc-2a436bed4826"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requires_grad: False\n","Requires_grad: True, False\n"]}],"source":["'''\n","By default, all tensors with requires_grad=True are tracking their computational \n","history and support gradient computation. However, there are some cases when we do \n","not need to do that, for example, when we have trained the model and just want to \n","apply it to some input data, i.e. we only want to do forward computations through the\n","network. We can stop tracking computations by surrounding our computation code with \n","torch.no_grad() block:\n","'''\n","with torch.no_grad():\n","    output = torch.matmul(input, weights) + biases\n","print(f'Requires_grad: {output.requires_grad}')\n","\n","output = torch.matmul(input, weights) + biases\n","out = output.detach()\n","print(f'Requires_grad: {output.requires_grad}, {out.requires_grad}')"]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":213,"status":"ok","timestamp":1638895561381,"user":{"displayName":"Andriy Sorokin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08930791557530031808"},"user_tz":-180},"id":"0CNg03lQw53k","outputId":"39bbb025-d5ad-41e6-d3ff-a539ee5b2023"},"outputs":[{"name":"stdout","output_type":"stream","text":["[True, True, True, False, True]\n","[True, True, True, False, False]\n"]}],"source":["a = torch.rand(3, requires_grad=True)\n","b = torch.rand(3, requires_grad=True)\n","c = torch.rand(3, requires_grad=True)\n","with torch.no_grad():\n","  d = a + b\n","  with torch.enable_grad():\n","    out = c * 0.1 + d\n","#print(list(map(lambda x: x.requires_grad, [a,b,c,d,out])))\n","out.sum().backward()\n","print(list(map(lambda x: x.requires_grad, [a,b,c,d,out])))\n","\n","@torch.enable_grad()\n","def foo(x):\n","    return x * 0.1\n","with torch.no_grad():\n","    d = a + b\n","    out = foo(c) + d\n","#out.sum().backward()\n","print(list(map(lambda x: x.requires_grad, [a,b,c,d,out])))\n","'''There are reasons you might want to disable gradient tracking:\n","To mark some parameters in your neural network as frozen parameters. \n","This is a very common scenario for finetuning a pretrained network\n","To speed up computations when you are only doing forward pass,\n","because computations on tensors that do not track gradients would be more efficient'''"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":["import torch \n","\n","a = torch.randn((3,3), requires_grad = True)\n","\n","w1 = torch.randn((3,3), requires_grad = True)\n","w2 = torch.randn((3,3), requires_grad = True)\n","w3 = torch.randn((3,3), requires_grad = True)\n","w4 = torch.randn((3,3), requires_grad = True)\n","\n","b = w1*a \n","c = w2*a\n","\n","d = w3*b + w4*c \n","\n","L = (10 -d)\n","\n","L.backward()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Mh7CRHlw57z"},"outputs":[],"source":["import math\n","import torch\n","dtype = torch.float\n","device = torch.device('cpu')\n","#device = torch.device(\"cuda:0\") \n","\n","'''\n","We will use a problem of fitting y=sin(x) with a third order polynomial as our running example. \n","The network will have four parameters, and will be trained with gradient descent to fit random data by \n","minimizing the Euclidean distance between the network output and the true output.\n","Polynom: a + b*x + c*x**2 + d*x**3 \n","'''\n","x = torch.linspace(-math.pi, math.pi, 3000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","params_num = 4\n","params = [torch.randn((), device=device, dtype=dtype) for i in range(params_num)]\n","# params[0] = torch.randn((), device=device, dtype=dtype)\n","# params[1] = torch.randn((), device=device, dtype=dtype)\n","# params[2] = torch.randn((), device=device, dtype=dtype)\n","# params[3] = torch.randn((), device=device, dtype=dtype)\n","\n","from functools import reduce\n","learning_rate = 1e-6\n","for t in range(2000):\n","    # Forward pass: compute predicted y\n","    #y_pred = params[0] + params[1] * x + params[2] * x ** 2 + params[3] * x ** 3\n","    y_pred = reduce(lambda x,y: x+y, [p*x**mod for mod, p in enumerate(params)]) \n","    \n","    # Compute and print loss\n","    loss = (y_pred - y).pow(2).sum().item()\n","    if t % 100 == 99:\n","        print(t, loss)\n","\n","    # Backprop to compute gradients of parameters with respect to loss\n","    grad_y_pred = 2.0 * (y_pred - y)\n","    grads = [(grad_y_pred*x**mod).sum() for mod, i in enumerate(range(params_num))]\n","    # params[0] = grad_y_pred.sum()\n","    # params[1] = (grad_y_pred * x).sum()\n","    # params[2] = (grad_y_pred * x ** 2).sum()\n","    # params[3] = (grad_y_pred * x ** 3).sum()\n","\n","    # Update weights using gradient descent\n","    for id, p in enumerate(params):\n","        p-=learning_rate * grads[id]\n","        \n","    # params[0] -= learning_rate * grads[0]\n","    # params[1] -= learning_rate * grads[1]\n","    # params[2] -= learning_rate * grads[2]\n","    # params[3] -= learning_rate * grads[3]\n","\n","[print(f'Result: {p.item()}') for p in params]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-L7_KC0MWhS"},"outputs":[],"source":["import math\n","import torch\n","dtype = torch.float\n","device = torch.device('cpu')\n","#device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n","\n","x = torch.linspace(-math.pi, math.pi, 3000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","params_num = 4\n","params = [torch.randn((), device=device, dtype=dtype, requires_grad=True) for i in range(params_num)]\n","\n","from functools import reduce\n","learning_rate = 1e-4\n","criterion = torch.nn.MSELoss(reduce='sum')\n","optim = torch.optim.Adam(params, lr=learning_rate)\n","\n","for t in range(3000):\n","    optim.zero_grad()\n","    # Forward pass: compute predicted y\n","    #y_pred = params[0] + params[1] * x + params[2] * x ** 2 + params[3] * x ** 3\n","    y_pred = reduce(lambda x,y: x+y, [p*x**mod for mod, p in enumerate(params)]) \n","    \n","    # Compute and print loss\n","    loss = criterion(y_pred, y)\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","   \n","    # Backprop to compute gradients of parameters with respect to loss\n","    loss.backward()\n","    # Update weights using gradient descent\n","    optim.step()\n","\n","[print(f'Result: {p.item()}') for p in params]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XkgtoLbbMWkJ"},"outputs":[],"source":["import torch\n","import math\n","\n","'''\n","In PyTorch we can easily define our own autograd operator by defining a subclass of torch.autograd.\n","Function and implementing the forward and backward functions. We can then use our new autograd operator \n","by constructing an instance and calling it like a function, passing Tensors containing input data.\n","Our function is 0.5 * (5 * x ** 3 - 3 * x ** 2)\n","'''\n","class CustomPolynomial(torch.autograd.Function):\n","    \"\"\"\n","    We can implement our own custom autograd Functions by subclassing\n","    torch.autograd.Function and implementing the forward and backward passes\n","    which operate on Tensors.\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input):\n","        \"\"\"\n","        In the forward pass we receive a Tensor containing the input and return\n","        a Tensor containing the output. ctx is a context object that can be used\n","        to stash information for backward computation. You can cache arbitrary\n","        objects for use in the backward pass using the ctx.save_for_backward method.\n","        \"\"\"\n","        ctx.save_for_backward(input)\n","        return 0.5 * (5 * input ** 3 - 3 * input)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        \"\"\"\n","        In the backward pass we receive a Tensor containing the gradient of the loss\n","        with respect to the output, and we need to compute the gradient of the loss\n","        with respect to the input.\n","        \"\"\"\n","        input, = ctx.saved_tensors\n","        return grad_output * 1.5 * (5 * input ** 2 - 1)\n","\n","\n","class Polynomial3(torch.nn.Module):\n","    def __init__(self):\n","        \"\"\"\n","        In the constructor we instantiate four parameters and assign them as\n","        member parameters.\n","        Create random Tensors for weights. For this example, we need\n","        4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n","        not too far from the correct result to ensure convergence.\n","        Setting requires_grad=True indicates that we want to compute gradients with\n","        respect to these Tensors during the backward pass.\n","        \"\"\"\n","        super().__init__()\n","        self.a = torch.nn.Parameter(torch.randn((), device=device, dtype=dtype, requires_grad=True)*0.01)\n","        self.b = torch.nn.Parameter(torch.randn((), device=device, dtype=dtype, requires_grad=True)*0.01)\n","        self.c = torch.nn.Parameter(torch.randn((), device=device, dtype=dtype, requires_grad=True)*0.01)\n","        self.d = torch.nn.Parameter(torch.randn((), device=device, dtype=dtype, requires_grad=True)*0.01)\n","        '''\n","        P3 using our custom autograd operation.\n","        To apply our Function, we use Function.apply method. We alias this as 'P3'.\n","        '''\n","        self.operation = CustomPolynomial.apply\n","\n","    def forward(self, x):\n","        \"\"\"\n","        In the forward function we accept a Tensor of input data and we must return\n","        a Tensor of output data. We can use Modules defined in the constructor as\n","        well as arbitrary operators on Tensors.\n","        \"\"\"\n","        return self.a + self.b * self.operation(self.c + self.d * x)\n","\n","    def string(self):\n","        \"\"\"\n","        Just like any class in Python, you can also define custom method on PyTorch modules\n","        \"\"\"\n","        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n","\n","\n","dtype = torch.float\n","device = torch.device(\"cuda\")\n","\n","# Create Tensors to hold input and outputs.\n","# By default, requires_grad=False, which indicates that we do not need to\n","# compute gradients with respect to these Tensors during the backward pass.\n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","learning_rate = 1e-4\n","model = Polynomial3().to(device)\n","criterion = torch.nn.MSELoss(reduction='sum')\n","optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","sheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[25000], gamma=0.1)\n","\n","for t in range(30000):\n","    # Zero gradients in optimizer\n","    optim.zero_grad()\n","    # Forward pass: compute predicted y using operations; we compute\n","    y_pred =  model(x) \n","    # Compute and print loss\n","    loss = criterion(y_pred, y)\n","    if t % 1000 == 999:\n","        print(t, loss.item())\n","    # Use autograd to compute the backward pass.\n","    loss.backward()\n","    # Update weights using gradient descent\n","    sheduler.step()\n","    optim.step()\n","\n","print(model.string())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yO-nhbG1MWvh"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMcXX5+hGy/O0Yav4jDc4x2","collapsed_sections":[],"name":"Untitled1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}