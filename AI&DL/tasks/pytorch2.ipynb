{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UDslJnIGDdm-"},"outputs":[],"source":["!mkdir cifar && cd cifar && wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","!tar -xvf cifar/cifar-10-python.tar.gz -C cifar/ --no-same-owner\n","!mkdir cifar/data\n","\n","import os\n","import platform\n","import numpy as np\n","from PIL import Image\n","from six.moves import cPickle as pickle\n","\n","def unpickle(file):\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","def load_pickle(f):\n","    return  pickle.load(f, encoding='latin1')\n","\n","def load_CIFAR_batch(filename):\n","    \"\"\" load single batch of cifar \"\"\"\n","    with open(filename, 'rb') as f:\n","        datadict = load_pickle(f)\n","        X = datadict['data']\n","        Y = datadict['labels']\n","        N = datadict['filenames']\n","        X = X.reshape(10000,3072)\n","        Y = np.array(Y)\n","        return X, Y, N\n","\n","\n","path = 'cifar'\n","location = os.path.join(path, 'cifar-10-batches-py/test_batch')\n","x, y, n = load_CIFAR_batch(location)\n","\n","for idx, file in enumerate(x):\n","    img = Image.fromarray(file.reshape((3,32,32)).transpose((1,2,0)))\n","    if not os.path.exists(os.path.join(path, 'data', str(y[idx]))):\n","        os.makedirs(os.path.join(path, 'data', str(y[idx])))\n","    img.save(os.path.join(path, 'data', str(y[idx]), str(n[idx])[:-4] + '.bmp'), format='BMP') \n","print(f\"Completed! {[len([f for f in os.listdir(os.path.join('/content/cifar/data/', str(i)))]) for i in range(10)]}\")\n","\n","\n","[os.makedirs(os.path.join('/content/cifar/test_data/', folder)) for folder in ['planes', 'cars', 'val/planes', 'val/cars', 'train/planes', 'train/cars']]\n","!cp -r cifar/data/0/* cifar/test_data/planes && cp -r cifar/data/1/* cifar/test_data/cars\n","!ls -rt /content/cifar/test_data/planes/* | head -200 | xargs mv -t /content/cifar/test_data/val/planes\n","!ls -rt /content/cifar/test_data/planes/* | head -800 | xargs mv -t /content/cifar/test_data/train/planes\n","!ls -rt /content/cifar/test_data/cars/* | head -200 | xargs mv -t /content/cifar/test_data/val/cars\n","!ls -rt /content/cifar/test_data/cars/* | head -800 | xargs mv -t /content/cifar/test_data/train/cars\n","!rm -rf /content/cifar/test_data/cars/ && rm -rf /content/cifar/test_data/planes/\n","print(f\"Completed! {[len([f for f in os.listdir(os.path.join('/content/cifar/test_data', str(i)))]) for i in ['val/planes', 'val/cars', 'train/planes', 'train/cars']]}\")"]},{"cell_type":"markdown","source":["# Новый раздел"],"metadata":{"id":"irUU7UdLDtcn"}},{"cell_type":"markdown","metadata":{"id":"VCxj-yw_DdnC"},"source":["1.b Загрузить один из известных наборов данных (Cifar, Mnist, ImageNet...) из torchvision. Далее работать с ним при помощи инструментов pyTorch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uf2rn74tDdnD"},"outputs":[],"source":["from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch\n","import os\n","\n","path = './1_b_torchvision'\n","if not os.path.exists(path):\n","    os.makedirs(path)\n","\n","trans = transforms.Compose(([\n","    transforms.PILToTensor(),\n","    transforms.ConvertImageDtype(torch.float),\n","    ]))\n","data = datasets.CIFAR100(path, train=True, transform=trans, download=True)\n","dloader = DataLoader(data, batch_size=4, shuffle=True, num_workers=2) \n","class_names = data.classes\n","print(data.classes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9G8PYGODdnE"},"outputs":[],"source":["import torchvision \n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def imshow(inp, title=None):\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    #inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UW-PLrtEDdnE"},"outputs":[],"source":["# Get a batch of training data\n","inputs, classes = next(iter(dloader))\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs)\n","imshow(out, title=[class_names[x] for x in classes])"]},{"cell_type":"markdown","metadata":{"id":"c8SVI3WtDdnF"},"source":["1.a Скачать один из известных наборов данных (Cifar, Mnist, ImageNet...) в оригинальном виде:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WRbnDddODdnF"},"outputs":[],"source":["def download(url, filename, chunk_size=1024):\n","    with open(filename, \"wb\") as fh:\n","        with urllib.request.urlopen(urllib.request.Request(url)) as response: \n","            with tqdm(total=response.length) as pbar:\n","                for chunk in iter(lambda: response.read(chunk_size), \"\"):\n","                    if not chunk:\n","                        break\n","                    pbar.update(chunk_size)\n","                    fh.write(chunk)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRS0fyWSDdnG"},"outputs":[],"source":["import urllib\n","from tqdm import tqdm\n","import pickle\n","import os\n","\n","path = './1_a_onw_tools'\n","if not os.path.exists(path):\n","    os.makedirs(path)\n","\n","download('https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz', os.path.join(path, 'cifar_10.tar.gz'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rd7gAcKDdnH"},"outputs":[],"source":["import tarfile\n","my_tar = tarfile.open(os.path.join(path, 'cifar_10.tar.gz'))\n","my_tar.extractall(path) \n","my_tar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AK232YmJDdnI"},"outputs":[],"source":["def get_data(path_to_file: str):\n","    data: Any = []\n","    targets = []\n","\n","    file_path = os.path.join(path_to_file)\n","    with open(file_path, 'rb') as f:\n","        entry = pickle.load(f, encoding='latin1')\n","        data.append(entry['data'])\n","        if 'labels' in entry:\n","            targets.extend(entry['labels'])\n","        else:\n","            targets.extend(entry['fine_labels'])\n","\n","    data = np.vstack(data).reshape(-1, 3, 32, 32) \n","    data = data.transpose((0, 2, 3, 1))  # convert to HWC\n","    return data, np.array(targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ijv8saquDdnI"},"outputs":[],"source":["img, lbl = get_data(os.path.join(path, 'cifar-10-batches-py/test_batch'))\n","print(f'DATA: {img.shape}, {lbl.shape}')\n","class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"]},{"cell_type":"markdown","metadata":{"id":"NpsLYIqSDdnJ"},"source":["Hаписать собственный класс Dataset, так чтобы выход был совместим с torch.utils.data.Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VfAmBUIqDdnJ"},"outputs":[],"source":["class CustomDataset():\n","    def __init__(self, images, labels):\n","        self.labels = labels\n","        self.images = self.prepare_image(images)\n","\n","    def prepare_image(self, img):\n","        img = img.transpose((0,3,1,2))\n","        img = img/255.\n","        return img\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        label = self.labels[idx]\n","        images = self.images[idx]\n","        return images, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0EUqM1sDdnK"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","dataset = CustomDataset(img, lbl)\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1W87Mc6RDdnK"},"outputs":[],"source":["# Get a batch of training data\n","inputs, classes = next(iter(dataloader))\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs)\n","imshow(out, title=[class_names[x] for x in classes])"]},{"cell_type":"markdown","metadata":{"id":"Jvixv_wJDdnK"},"source":["2.а Попробовать предварительно обработать данные, так, чтобы их можно было загрузить при помощи torchvision.Datasets (torchvision.datasets.DatasetFolder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yu2I3balDdnK"},"outputs":[],"source":["import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZNjxPa6DdnL"},"outputs":[],"source":["data_transforms = {\n","    'train': transforms.Compose([\n","        #transforms.RandomResizedCrop(32),\n","        #transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        #transforms.Resize(32),\n","        #transforms.CenterCrop(32),\n","        transforms.ToTensor(),\n","        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","data_dir = 'cifar/test_data/'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n","                                             shuffle=True, num_workers=2)\n","              for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6gso7jRDdnL"},"outputs":[],"source":["# Get a batch of training data\n","inputs, classes = next(iter(dataloaders['train']))\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs)\n","imshow(out, title=[class_names[x] for x in classes])"]},{"cell_type":"markdown","metadata":{"id":"FCLKbWiDDdnL"},"source":["Преобразования проводимые с изображениями"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CsrCiqWkDdnM"},"outputs":[],"source":["import numpy as np\n","from skimage.transform import rescale, rotate\n","import random\n","import numbers\n","import os\n","from PIL import Image, ImageOps\n","import collections\n","\n","class Compose:\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, data):\n","        for t in self.transforms:\n","           data = t(data)\n","        return data\n","\n","\n","class Scale(object):\n","\n","    def __init__(self, scale):\n","        self.scale = scale\n","\n","    def __call__(self, sample):\n","        image, label = sample\n","        image = np.asarray(image) #Image.fromarray\n","        img_size = image.shape[0]\n","\n","        scale = np.random.uniform(low=1.0 - self.scale, high=1.0 + self.scale)\n","        \n","        image = rescale(\n","            image,\n","            (scale, scale),\n","            multichannel=True,\n","            preserve_range=True,\n","            mode=\"constant\",\n","            anti_aliasing=False,\n","        )\n","\n","        if scale < 1.0:\n","            diff = (img_size - image.shape[0]) / 2.0\n","            padding = ((int(np.floor(diff)), int(np.ceil(diff))),) * 2 + ((0, 0),)\n","            image = np.pad(image, padding, mode=\"constant\", constant_values=0)\n","        else:\n","            x_min = (image.shape[0] - img_size) // 2\n","            x_max = x_min + img_size\n","            image = image[x_min:x_max, x_min:x_max, ...]\n","\n","        image = Image.fromarray(image.astype('uint8'))\n","        return image, label\n","\n","\n","class Rotate(object):\n","    def __init__(self, angle):\n","        self.angle = angle\n","        self.topil = toPIL()\n","\n","    def __call__(self, sample):\n","        image, label = sample\n","        angle = np.random.uniform(low=-self.angle, high=self.angle)\n","        image, label = self.topil(sample)\n","        image = image.rotate(angle=angle)    \n","        return image, label\n","\n","\n","class Normalize(object):\n","    def __init__(self, std, mean):\n","        self.std = std\n","        self.mean = mean\n","\n","    def __call__(self, sample):\n","        image, label = sample\n","        image = np.array(image, dtype=np.float32)\n","        image = image / 255\n","        image[:,:,0] = (image[:,:,0] - self.mean[0]) / self.std[0]\n","        image[:,:,1] = (image[:,:,1] - self.mean[1]) / self.std[1]\n","        image[:,:,2] = (image[:,:,2] - self.mean[2]) / self.std[2]\n","        image = Image.fromarray((image*255).astype('uint8'))\n","        return image, label\n","\n","\n","class HorizontalFlip(object):\n","    def __init__(self, flip_prob):\n","        self.flip_prob = flip_prob\n","\n","    def __call__(self, sample):\n","        image, label = sample\n","        if np.random.rand() < self.flip_prob:\n","            return image, label\n","        img = image.transpose(method=Image.FLIP_TOP_BOTTOM)\n","        return img, label\n","\n","\n","class RandomCrop(object):\n","    def __init__(self, size, padding=0):\n","        if isinstance(size, numbers.Number):\n","            self.size = (int(size), int(size))\n","        else:\n","            self.size = size\n","        self.padding = padding\n","\n","    @staticmethod\n","    def get_params(img, output_size):\n","        w, h = img.size\n","        th, tw = output_size\n","        if w == tw and h == th:\n","            return img\n","        if w > tw and h > th:\n","            i = random.randint(0, th)\n","            j = random.randint(0, tw)\n","        else:\n","            i = random.randint(0, h - th)\n","            j = random.randint(0, w - tw)\n","        return i, j, th, tw\n","\n","    def pad(self, img, padding, fill=0):\n","        if not isinstance(img, Image.Image):\n","            raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n","\n","        if not isinstance(padding, (numbers.Number, tuple)):\n","            raise TypeError('Got inappropriate padding arg')\n","        if not isinstance(fill, (numbers.Number, str, tuple)):\n","            raise TypeError('Got inappropriate fill arg')\n","\n","        if isinstance(padding, collections.Sequence) and len(padding) not in [2, 4]:\n","            raise ValueError(\"Padding must be an int or a 2, or 4 element tuple, not a \" +\n","                            \"{} element tuple\".format(len(padding)))\n","\n","        return ImageOps.expand(img, border=padding, fill=fill)\n","\n","    def __call__(self, sample):\n","        image, label = sample\n","        if self.padding > 0:\n","            image = self.pad(image, self.padding)\n","        i, j, h, w = self.get_params(image, self.size)\n","        img = image.crop((i, j, i+h, j+w))\n","        return img, label\n","\n","\n","class toNumpy():\n","    def __init__(self):\n","        pass\n","    def __call__(self, sample):\n","        image, label = sample\n","        if not isinstance(image, (np.ndarray, np.generic) ):\n","            image = np.asarray(image)\n","        image = np.transpose(image, (2,0,1))\n","        return image, label\n","\n","    \n","class toPIL():\n","    def __init__(self):\n","        pass\n","    def __call__(self, sample):\n","        image, label = sample        \n","        if isinstance(image, (Image.Image)):\n","            return image, label\n","        elif isinstance(image, (np.ndarray, np.generic)):\n","            if image.shape[2] != 3:\n","                image = image.transpose((1,2,0))\n","            image = Image.fromarray(image.astype('uint8'))\n","            return image, label\n","        else:\n","            raise TypeError(f'Unsupported type {type(image)} for this operation')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2ochMt6DdnM"},"outputs":[],"source":["!mkdir image && cd image && wget https://lh3.googleusercontent.com/sZl-o9TGNYXucrPNHesxaMLXajhcPYqw43zojzHJ-y1yYYeQaNtJMrPUQImKgak3paKOMbEm0Av0e5bKG8_z31m1xVGN8J3x-EIAfgeETIhrLxwsw7xWEGstIuwyKYuHjOLFPCcvqIBY944PWFHBhgSEsVERXJljVEwPTD1xpJHhi5gHieiZcjl-rJ734bFiDxG1GzTxkX5nRc9lkRhtNHMdHDOSswMU-dgp8itMF8lTdEztOYz_bE_8H2FxN5NtCBmeOvxTi7f31wM2zrAE7oOzzeVy1_hYLFAWuXJ0CNqDfc-J-Ui9HY1RILj9Z1nYvEKGGDSTiT3tzysWHO9Vn6rXDFKE9TUGNE1_Z9_EaQ7B_HIU_z7oq2Hmmikl1Ap5t7N_pEI44ZhUGbIVirHKJyvc2LgtJgczCeNEgH7SnvvYBCM-OMWX5MnB949rXYn678iZyz7Q16wjPMevr1IQU4zfdOE2xoJQi8x3FhpD2-3moUoilZRBR5OQi-KII4hRYrEaRFnAiGqYeFz3Rqwx_Yw97kROwh2AnfhT03700AHkFqmYobTXjq0Q3IQpjcYQZ3vbtw=w800-h400-p && mv ./* image.jpg\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4djvyBxDdnM"},"outputs":[],"source":["import random\n","img = Image.open('./image/image.jpg')\n","label = np.array([1])\n","# transform = Compose([RandomCrop(200, padding=50)])\n","# image, label = transform((img, label))\n","# transform = Compose([HorizontalFlip(0.5)])\n","# image, label = transform((img, label))\n","# transform = Compose([Rotate(90)])\n","# image, label = transform((img, label))\n","# transform = Compose([Scale(0.9)])\n","# image, label = transform((img, label))\n","# transform = Compose([Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n","# image, label = transform((img, label))                    \n","# transform = Compose([toNumpy()])                    \n","# image, label = transform((img, label))\n","\n","# transform = Compose([RandomCrop(200, padding=0),\n","#                     HorizontalFlip(0.5),\n","#                     Rotate(90),\n","#                     Scale(0.9),\n","#                     Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","#                     ])\n","# image, label = transform((img, label))\n","\n","display(image)"]},{"cell_type":"markdown","metadata":{"id":"6sydkWq-DdnN"},"source":["Загрузчики данных"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMBZkCjpDdnN"},"outputs":[],"source":["def get_data(path_to_file: str):\n","    data: Any = []\n","    targets = []\n","\n","    file_path = os.path.join(path_to_file)\n","    with open(file_path, 'rb') as f:\n","        entry = pickle.load(f, encoding='latin1')\n","        data.append(entry['data'])\n","        if 'labels' in entry:\n","            targets.extend(entry['labels'])\n","        else:\n","            targets.extend(entry['fine_labels'])\n","\n","    data = np.vstack(data).reshape(-1, 3, 32, 32) \n","    data = data.transpose((0, 2, 3, 1))  # convert to HWC\n","    return data, np.array(targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MwIAYr8jDdnN"},"outputs":[],"source":["import pickle\n","import numpy as np\n","\n","path = './cifar'\n","imgs, lbls = get_data(os.path.join(path, 'cifar-10-batches-py/test_batch'))\n","print(f'DATA: {imgs.shape}, {lbls.shape}')\n","class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7BJSkHCjDdnO"},"outputs":[],"source":["class CustomDataset():\n","    def __init__(self, images, labels, transforms=None):\n","        self.labels = labels\n","        self.images = images #self.prepare_image(images)\n","        self.transforms = transforms\n","\n","    def prepare_image(self, img):\n","        img = img.transpose((2,0,1))\n","        img = img/255.\n","        return img\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        label = self.labels[idx]\n","        image = self.images[idx]\n","        if self.transforms is not None:\n","            image, label = self.transforms([image, label])\n","            image = image / 255\n","        else:\n","            image = self.prepare_image(image)\n","        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.int32).unsqueeze(-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vRVmhnlDdnO"},"outputs":[],"source":["def own_collate(batch):\n","    if isinstance(batch, list):\n","        for el in batch:\n","            if isinstance(el, tuple) and len(el) == 2:\n","                pass\n","            else:\n","                raise TypeError(f'Unsupported element {type(el)} for length {len(el)}')\n","        elements = list(map(lambda x: list(x), zip(*batch)))\n","    return elements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEWLR3aeDdnO"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","import torchvision\n","import torch\n","\n","transform = Compose([Rotate(90),\n","                     toNumpy()])\n","\n","dataset = CustomDataset(imgs, lbls, transforms=transform)\n","\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0, sampler=None,\n","                        batch_sampler=None, collate_fn=own_collate, pin_memory=False, drop_last=False, timeout=0,\n","                        worker_init_fn=None, prefetch_factor=2, persistent_workers=False) \n","\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloader))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs)\n","imshow(out, title=[class_names[x] for x in classes])"]},{"cell_type":"markdown","source":["Tensorboard"],"metadata":{"id":"LRiite5BfqBQ"}},{"cell_type":"code","source":["%load_ext tensorboard\n","\n","import torch\n","import torchvision\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets\n","\n","# Writer will output to ./runs/ directory by default\n","writer = SummaryWriter()\n","\n","# model = torch.nn.Sequential(torch.nn.Conv2d(3, 64, 3),\n","#                           torch.nn.BatchNorm2d(64),\n","#                           torch.nn.ReLU(),\n","#                           torch.nn.Conv2d(64, 32, 3),\n","#                           torch.nn.BatchNorm2d(32),\n","#                           torch.nn.ReLU(),\n","#                           torch.nn.Conv2d(32, 16, 3),\n","#                           torch.nn.BatchNorm2d(16),\n","#                           torch.nn.ReLU()\n","#                           )\n","model = torchvision.models.resnet50()\n","\n","image = torch.stack(inputs)\n","grid = torchvision.utils.make_grid(image)\n","writer.add_image('images', grid, 0)\n","writer.add_graph(model, image)\n","writer.close()"],"metadata":{"id":"J4U4Q5z3fuGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir=runs"],"metadata":{"id":"gOFrSlZNHLLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","\n","writer = SummaryWriter()\n","\n","for n_iter in range(100):\n","    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n","    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n","    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n","    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"],"metadata":{"id":"UqNIg37ZpHHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir=runs"],"metadata":{"id":"cGYx2eG-f2UV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter()\n","r = 5\n","for i in range(100):\n","    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n","                                    'xcosx':i*np.cos(i/r),\n","                                    'tanx': np.tan(i/r)}, i)\n","writer.close()"],"metadata":{"id":"RmtytEceBwO7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir=runs"],"metadata":{"id":"8iKQiDVMBzbr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","import torch\n","import os\n","import tensorflow as tf\n","import tensorboard as tb\n","tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n","\n","writer = SummaryWriter()\n","\n","path = './1_b_torchvision'\n","if not os.path.exists(path):\n","    os.makedirs(path)\n","\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","    ])\n","\n","data = datasets.CIFAR100(path, train=True, transform=transform, download=True)\n","dloader = DataLoader(data, batch_size=4, shuffle=True, num_workers=2) \n","class_names = data.classes\n","print(data.classes)\n","\n","\n","def select_n_random(data, labels, n=100):\n","    '''\n","    Selects n random datapoints and their corresponding labels from a dataset\n","    '''\n","    assert len(data) == len(labels)\n","    perm = torch.randperm(len(data))\n","    return torch.tensor(data)[perm][:n], torch.tensor(labels)[perm][:n]\n","\n","# select random images and their target indices\n","images, labels = select_n_random(data.data, data.targets)\n","\n","images = images.transpose(1,3).transpose(2,3)/255.\n","# get the class labels for each image\n","class_labels = [class_names[lab] for lab in labels]\n","\n","# log embeddings\n","print(f'Is data in image contiguous: {images.is_contiguous()}')\n","features = images.view(-1, 32 * 32 * 3) #.contiguous().\n","writer.add_embedding(features,\n","                    metadata=class_labels,\n","                    label_img=images) \n","writer.close()"],"metadata":{"id":"XdfMR60MIv0u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir=runs"],"metadata":{"id":"l3LQkA94Ivxf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf ./runs/*"],"metadata":{"id":"mnpJMj249-rk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import torch\n","from torch.utils.tensorboard import SummaryWriter\n","\n","class Model:\n","  def __init__(self):\n","      self.params = [torch.randn((), device=device, dtype=dtype, requires_grad=True) for i in range(params_num)]\n","\n","  def get_params(self):\n","      return self.params\n","\n","  def __call__(self, x):\n","      return reduce(lambda x,y: x+y, [p*x**mod for mod, p in enumerate(self.params)])  \n","\n","writer = SummaryWriter()\n","\n","dtype = torch.float\n","device = torch.device('cpu')\n","\n","x = torch.linspace(-math.pi, math.pi, 3000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","params_num = 4\n","model = Model()\n","\n","from functools import reduce\n","learning_rate = 1e-4\n","running_loss = 0.0\n","criterion = torch.nn.MSELoss()\n","optim = torch.optim.Adam(model.get_params(), lr=learning_rate)\n","for t in range(10000):\n","    optim.zero_grad()\n","    y_pred = model(x) \n","    loss = criterion(y_pred, y)\n","    loss.backward()   \n","    optim.step()\n","\n","    running_loss += loss.item()\n","    if t % 100 == 99:\n","        writer.add_scalar('training loss',\n","                                running_loss/100,\n","                                2000 + t)\n","        writer.add_scalar('loss',\n","                                loss.item(),\n","                                2000 + t)\n","        running_loss = 0.0\n","\n","print(f'Training is completed')\n"],"metadata":{"id":"jxSJbXpdIvuX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir=runs"],"metadata":{"id":"mLXe7gFlIvrm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hILHpwijIvov"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"qPqhbnDxIvff"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"orig_nbformat":2,"colab":{"name":"pytorch2.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}